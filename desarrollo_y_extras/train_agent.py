from stable_baselines3 import PPO
from ocr_environment import OCREnvironment
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Usando dispositivo: {device}")

# 1. Crear el entorno personalizado (con la nueva lÃ³gica de diagnÃ³stico)
env = OCREnvironment()

# 2. Crear el agente PPO
model = PPO(
    "MlpPolicy", 
    env, 
    verbose=1, 
    tensorboard_log="sistema_control_adaptativo/logs_tensorboard/", 
    device=device
)

print("\nðŸš€ Iniciando re-entrenamiento del agente con diagnÃ³stico de entorno...")

# 3. Entrenar el agente por mÃ¡s tiempo
model.learn(total_timesteps=50000)

print("\nâœ… Re-entrenamiento finalizado.")

# 4. Guardar el nuevo agente "experto"
model.save("sistema_control_adaptativo/agentes_entrenados/ppo_agente_experto")
print("\nðŸ’¾ Agente experto guardado en 'sistema_control_adaptativo/agentes_entrenados/'")